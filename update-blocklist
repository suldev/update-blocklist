#!/bin/python3

import os, glob
import sys
import requests
from datetime import datetime as dt
import argparse

# Add urls here
block_lists = []
white_list_urls = []

# Application configuration
line_prefix = 'local=/'
line_postfix= '/\n'
file_search_key = '*.txt'
time_format = '%Y-%m-%d %H:%M:%S'
out_file = 'blocklist.conf'

if len(sys.argv) > 1:
    temp_path = sys.argv[len(sys.argv) - 1]
else:
    temp_path = './'
if temp_path.startswith('./'):
    temp_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), temp_path[2:])
all_lines = []

def pull_blocklists(save_path):
    for url in block_lists:
        try:
            response = requests.get(url)

            response.raise_for_status()

            save_file = os.path.join(save_path, url[url.rfind('/') + 1:])

            with open(save_file, 'wb') as f:
                f.write(response.content)

        except requests.exceptions.HTTPError as errh:
            print(f"HTTP Error: {errh}")
        except requests.exceptions.ConnectionError as errc:
            print(f"Error Connecting: {errc}")
        except requests.exceptions.Timeout as errt:
            print(f"Timeout Error: {errt}")
        except requests.exceptions.RequestException as err:
            print(f"Other Error: {err}")

def distill_blocklists(filter):
    all_lines = []
    files = glob.glob(filter)

    for file in files:
        with open(file,'r') as file_in:
            all_lines = file_in.readlines()

    return set(all_lines), len(all_lines)

def write_blocklist(out_file, contents):
    out_file.write("# suldev custom blocklist v0.1\n")
    out_file.write(f"# Published: {dt.now().strftime(time_format)}\n")
    out_file.write(f"# Unique Entries: {len(contents)}\n")
    for url in block_lists:
        out_file.writelines(f"# {url}\n")
    out_file.write("".join(sorted(contents, key=str.lower)))

def main():
    # Handle arguments
    parser = argparse.ArgumentParser(
        prog='Update Blocklist',
        description='Combines multiple blocklist files into a single dnsmasq configuration file',
    )
    parser.add_argument('path', help='Required. The working and output path')
    parser.add_argument('-w','--whitelist', default=None, type=argparse.FileType('r'), metavar='FILE', help='Specify a line-by-line whitelist file. Matching lines will be commented out')
    parser.add_argument('-W', default=None, type=argparse.FileType('r'), metavar='FILE', help='Specify a line-by-line whitelist file. Matching lines will be omitted')
    parser.add_argument('-i','--install', default=False, action='store_true', help='Install the configuration file and restart dnsmasq')
    args = parser.parse_args()

    # Check whitelist
    if args.whitelist is not None:
        temp = args.whitelist.readlines()
        for t in temp:
            white_list_urls.append(t.rstrip())
    print(args.install)
    quit()

    print(":: Warming up")
    os.mkdir(temp_path)

    print(":: Pulling blocklists")
    pull_blocklists(temp_path)

    print(":: Distilling blocklists")
    out_lines,all_lines_len = distill_blocklists(os.path.join(temp_path, file_search_key))
    filtered_list = [s for s in out_lines if s.startswith(line_prefix)]
    filtered_list = [s for s in filtered_list if s.endswith(line_postfix)]
    filtered_list = [s for s in filtered_list if not any(s == k for k in white_list_urls)]
    line_set_len = len(filtered_list)
    print(f" Total lines: {all_lines_len}")
    print(f" Final lines: {line_set_len}")
    print(f" Duplicates:  {all_lines_len - line_set_len}")
    
    print(":: Cleaning up")
    with open(os.path.join(temp_path, out_file),'w') as file_out:
        write_blocklist(file_out, filtered_list)
        
main()
