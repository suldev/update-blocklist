#!/bin/python3

import os, glob
import sys
import requests
from datetime import datetime as dt
import argparse
import shutil

# Add urls here
block_lists = []
white_list_urls = []

# Application configuration
line_prefix = 'local=/'
line_postfix= '/\n'
file_search_key = '*.txt'
time_format = '%Y-%m-%d %H:%M:%S'

if len(sys.argv) > 1:
    temp_path = sys.argv[len(sys.argv) - 1]
else:
    temp_path = './'
if temp_path.startswith('./'):
    temp_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), temp_path[2:])
all_lines = []

def pull_blocklists(save_path):
    for url in block_lists:
        try:
            response = requests.get(url)

            response.raise_for_status()

            save_file = os.path.join(save_path, url[url.rfind('/') + 1:])

            with open(save_file, 'wb') as f:
                f.write(response.content)

        except requests.exceptions.HTTPError as errh:
            print(f"HTTP Error: {errh}")
        except requests.exceptions.ConnectionError as errc:
            print(f"Error Connecting: {errc}")
        except requests.exceptions.Timeout as errt:
            print(f"Timeout Error: {errt}")
        except requests.exceptions.RequestException as err:
            print(f"Other Error: {err}")

def find_and_remove(url, lines, omit):
    urls = []
    output = []
    for line in lines:
        urls.append(line[len(line_prefix):-len(line_postfix)])
    if url not in urls:
        return lines
    for i in range(len(urls)):
        if urls[i] == url:
            if not omit:
                output.append("#" + lines[i])
        else:
            output.append(lines[i])

def distill_blocklists(filter, omit):
    all_lines = []
    files = glob.glob(filter)

    for file in files:
        with open(file,'r') as file_in:
            all_lines = file_in.readlines()

    unique_lines = set(all_lines)
    for entry in white_list_urls:
        sterilized_lines = find_and_remove(entry, unique_lines, omit)

    return sterilized_lines, len(all_lines)

def write_blocklist(out_file, contents):
    out_file.write("# suldev custom blocklist v0.1\n")
    out_file.write(f"# Published: {dt.now().strftime(time_format)}\n")
    out_file.write(f"# Unique Entries: {len(contents)}\n")
    for url in block_lists:
        out_file.writelines(f"# {url}\n")
    out_file.write("".join(sorted(contents, key=str.lower)))

def main():
    # Handle arguments
    parser = argparse.ArgumentParser(
        prog='Update Blocklist',
        description='Combines multiple blocklist files into a single dnsmasq configuration file',
    )
    wl_group = parser.add_argument_group('Whitelist', 'Provide a newline-separated list of urls to whitelist')
    exclusive_group = wl_group.add_mutually_exclusive_group()
    parser.add_argument('path', help='Required. The working and output path')
    parser.add_argument('-i', '--install', default=False, action='store_true', help='Install the configuration file and restart dnsmasq')
    parser.add_argument('-o', '--output', default='blocklist.conf', help='Output file name. Defaults to blocklist.conf')
    exclusive_group.add_argument('-w', default=None, type=argparse.FileType('r'), metavar='FILE', help='Matching lines will be commented out in the output file')
    exclusive_group.add_argument('-W', default=None, type=argparse.FileType('r'), metavar='FILE', help='Matching lines will be omitted from the output file')
    args = parser.parse_args()

    # Check whitelist
    if args.w is not None and args.W is not None:
        parser.print_help()
        print('EE Attributes whitelist and W cannot be used in the same context')
        quit()
    if args.w is not None:
        temp = args.w.readlines()
        for t in temp:
            white_list_urls.append(t.rstrip())
    omit = args.W is not None
    if args.W is not None:
        temp = args.W.readlines()
        for t in temp:
            white_list_urls.append(t.rstrip())
    out_file = args.output
    temp_path = args.path
    print(temp_path)

    print(":: Warming up")
    if os.path.exists(temp_path):
        print('EE Temporary path already exists')
        quit()
    os.mkdir(temp_path)

    print(":: Pulling blocklists")
    pull_blocklists(temp_path)

    print(":: Distilling blocklists")
    out_lines,all_lines_len = distill_blocklists(os.path.join(temp_path, file_search_key))
    filtered_list = [s for s in out_lines if s.startswith(line_prefix)]
    filtered_list = [s for s in filtered_list if s.endswith(line_postfix)]
    filtered_list = [s for s in filtered_list if not any(s == k for k in white_list_urls)]
    line_set_len = len(filtered_list)
    print(f" Total lines: {all_lines_len}")
    print(f" Final lines: {line_set_len}")
    print(f" Duplicates:  {all_lines_len - line_set_len}")
    
    print(":: Cleaning up")
    temp_file = os.path.join(temp_path, out_file)
    with open(temp_file,'w') as file_out:
        write_blocklist(file_out, filtered_list)
    if args.install:
        shutil.move(temp_file, )
        
main()
